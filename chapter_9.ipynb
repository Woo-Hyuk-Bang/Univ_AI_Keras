{"cells":[{"cell_type":"code","source":["# 9.2 이미지 분할 예제\n","# 필요한 라이브러리 설치\n","#!pip install tensorflow wget\n","\n","# 데이터 다운로드\n","import wget\n","import tarfile\n","\n","url = 'https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz'\n","wget.download(url)\n","url = 'https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz'\n","wget.download(url)\n","\n","with tarfile.open('images.tar.gz') as tar:\n","    tar.extractall()\n","\n","with tarfile.open('annotations.tar.gz') as tar:\n","    tar.extractall()\n","\n","# 데이터 전처리\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","\n","input_dir = 'images/'\n","target_dir = 'annotations/trimaps/'\n","img_size = (160, 160)\n","num_imgs = len(os.listdir(input_dir))\n","\n","input_img_paths = sorted([\n","    os.path.join(input_dir, fname)\n","    for fname in os.listdir(input_dir)\n","    if fname.endswith('.jpg')\n","])\n","target_img_paths = sorted([\n","    os.path.join(target_dir, fname)\n","    for fname in os.listdir(target_dir)\n","    if fname.endswith('.png') and not fname.startswith('.')\n","])\n","\n","# 리스트 길이 출력\n","print(f\"Number of input images: {len(input_img_paths)}\")\n","print(f\"Number of target images: {len(target_img_paths)}\")\n","\n","# 리스트의 몇 개 항목 출력\n","print(f\"Sample input image paths: {input_img_paths[:5]}\")\n","print(f\"Sample target image paths: {target_img_paths[:5]}\")\n","\n","def load_image(path, size):\n","    return img_to_array(load_img(path, target_size=size)) / 255.\n","\n","def load_mask(path, size):\n","    img = img_to_array(load_img(path, target_size=size, color_mode=\"grayscale\"))\n","    img[img == 1.0] = 0\n","    img[img == 2.0] = 1\n","    img[img == 3.0] = 2\n","    return img\n","\n","# num_imgs 값을 input_img_paths와 target_img_paths의 길이로 설정\n","num_imgs = min(len(input_img_paths), len(target_img_paths))\n","\n","x = np.zeros((num_imgs, img_size[0], img_size[1], 3), dtype=\"float32\")\n","y = np.zeros((num_imgs, img_size[0], img_size[1], 1), dtype=\"uint8\")\n","\n","for i in range(num_imgs):\n","    x[i] = load_image(input_img_paths[i], img_size)\n","    y[i] = load_mask(target_img_paths[i], img_size)\n","\n","# 데이터셋 나누기\n","val_samples = 1000\n","train_x, val_x = x[:-val_samples], x[-val_samples:]\n","train_y, val_y = y[:-val_samples], y[-val_samples:]\n","\n","# 모델 정의\n","from tensorflow.keras import layers\n","\n","def get_model(img_size, num_classes):\n","    inputs = tf.keras.Input(shape=img_size + (3,))\n","    x = layers.Conv2D(32, 3, padding=\"same\")(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.MaxPooling2D()(x)\n","    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.MaxPooling2D()(x)\n","    x = layers.Conv2D(128, 3, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Conv2DTranspose(64, 3, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Conv2DTranspose(32, 3, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Conv2D(num_classes, 1, activation=\"softmax\")(x)\n","    model = tf.keras.Model(inputs, x)\n","    return model\n","\n","# 모델 컴파일 및 훈련\n","model = get_model(img_size, 3)\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","callbacks = [\n","    tf.keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)\n","]\n","\n","history = model.fit(train_x, train_y, epochs=50, callbacks=callbacks, validation_data=(val_x, val_y))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":926},"id":"H5i8FwFQb3Bv","executionInfo":{"status":"error","timestamp":1717659479601,"user_tz":-540,"elapsed":90777,"user":{"displayName":"방우혁","userId":"03587500139386391947"}},"outputId":"5bfc3ce9-1aea-4fd4-fb7d-8cb50116f3c6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of input images: 7390\n","Number of target images: 7390\n","Sample input image paths: ['images/Abyssinian_1.jpg', 'images/Abyssinian_10.jpg', 'images/Abyssinian_100.jpg', 'images/Abyssinian_101.jpg', 'images/Abyssinian_102.jpg']\n","Sample target image paths: ['annotations/trimaps/Abyssinian_1.png', 'annotations/trimaps/Abyssinian_10.png', 'annotations/trimaps/Abyssinian_100.png', 'annotations/trimaps/Abyssinian_101.png', 'annotations/trimaps/Abyssinian_102.png']\n","Epoch 1/50\n"]},{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 969, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 160 and 40 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Squeeze, Cast_2)' with input shapes: [?,160,160], [?,40,40].\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5221a44ce22f>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m ]\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 969, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 160 and 40 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Squeeze, Cast_2)' with input shapes: [?,160,160], [?,40,40].\n"]}]},{"cell_type":"code","source":["# 9-3. 아키텍처\n","# 필요한 라이브러리 설치\n","# !pip install tensorflow\n","\n","# 모델 정의\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","def residual_block(x, filters, kernel_size=3):\n","    shortcut = x\n","    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.add([shortcut, x])\n","    x = layers.Activation('relu')(x)\n","    return x\n","\n","input_shape = (128, 128, 3)\n","num_classes = 10\n","\n","inputs = tf.keras.Input(shape=input_shape)\n","x = layers.Conv2D(64, 3, padding='same')(inputs)\n","x = layers.BatchNormalization()(x)\n","x = layers.Activation('relu')(x)\n","\n","x = residual_block(x, 64)\n","x = residual_block(x, 64)\n","\n","x = layers.MaxPooling2D()(x)\n","x = residual_block(x, 128)\n","x = residual_block(x, 128)\n","\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(num_classes, activation='softmax')(x)\n","\n","model = models.Model(inputs, x)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","model.summary()\n","\n","# 가상 데이터로 훈련 (실제 데이터로 교체 가능)\n","x_train = np.random.rand(100, 128, 128, 3)\n","y_train = np.random.randint(0, 10, 100)\n","\n","history = model.fit(x_train, y_train, epochs=10)\n"],"metadata":{"id":"XegvSWHecB8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 9.4 컨브넷이 학습한 것 해석하기\n","# 필요한 라이브러리 설치\n","# !pip install tensorflow\n","\n","# 모델 정의 및 학습 (간단한 예제)\n","import tensorflow as tf\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","model = VGG16(weights='imagenet')\n","\n","# 이미지 로드 및 전처리\n","img_path = 'elephant.jpg'  # 여기에 이미지 파일 경로를 입력하세요.\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","# 예측\n","preds = model.predict(x)\n","print('Predicted:', decode_predictions(preds, top=3)[0])\n","\n","# CAM 기법\n","class_output = model.output[:, np.argmax(preds[0])]\n","last_conv_layer = model.get_layer('block5_conv3')\n","\n","grads = tf.keras.backend.gradients(class_output, last_conv_layer.output)[0]\n","pooled_grads = tf.keras.backend.mean(grads, axis=(0, 1, 2))\n","iterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n","pooled_grads_value, conv_layer_output_value = iterate([x])\n","\n","for i in range(pooled_grads_value.shape[-1]):\n","    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n","\n","heatmap = np.mean(conv_layer_output_value, axis=-1)\n","\n","# Heatmap 시각화\n","heatmap = np.maximum(heatmap, 0)\n","heatmap /= np.max(heatmap)\n","plt.matshow(heatmap)\n","plt.show()\n","\n","# Heatmap을 원본 이미지에 덧붙이기\n","import cv2\n","\n","img = cv2.imread(img_path)\n","heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n","heatmap = np.uint8(255 * heatmap)\n","heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","\n","superimposed_img = heatmap * 0.4 + img\n","cv2.imwrite('elephant_cam.jpg', superimposed_img)\n"],"metadata":{"id":"00QFk4Tecans"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2OuOibyEt73FqU/mCa9UZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}