{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN34mMWh0R1cHz7focTJFhJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","합성곱 신경망(Convolutional Neural Network, CNN)은 주로 이미지 인식 및 패턴 인식에 사용되는 딥러닝 모델입니다. CNN은 이미지와 같은 다차원 데이터를 처리하는 데 특히 효과적입니다. 아래는 CNN의 주요 개념과 구성 요소에 대한 간단한 소개입니다.\n","\n","합성곱 연상\n","1. 완전 연결층과 합성곱 층 사이의 근본적인 차이는 다음과 같다\n","  1-1. Dense - 전역 패턴을 학습\n","  1-2. 합성곱 - 지역 패턴을 학습\n","  1-3. 이미지일 경우, 작은 2D 윈도우로 입력해서 패턴을 찾음\n","  1-4. 앞의 예에서 이 윈도우는 모두 3x3 크기\n"],"metadata":{"id":"uuiIiuYYH0QB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2nKvEoY7rw9","executionInfo":{"status":"ok","timestamp":1712654498839,"user_tz":-540,"elapsed":825,"user":{"displayName":"방우혁","userId":"03587500139386391947"}},"outputId":"a6e40582-d568-48e4-d246-7bf6c2f921c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d_6 (MaxPoolin  (None, 13, 13, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_7 (MaxPoolin  (None, 5, 5, 64)          0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 3, 3, 128)         73856     \n","                                                                 \n"," flatten_3 (Flatten)         (None, 1152)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                11530     \n","                                                                 \n","=================================================================\n","Total params: 104202 (407.04 KB)\n","Trainable params: 104202 (407.04 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# 8-1 간단한 컨브넷 만들기\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","inputs = keras.Input(shape=(28, 28, 1))\n","x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n","x = layers.Flatten()(x)\n","outputs = layers.Dense(10, activation=\"softmax\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","# 8-2 모델의 summary() 메서드 출력\n","model.summary()"]},{"cell_type":"code","source":["# 코드 8-3 MNIST 이미지에서 컨브넷 훈련하기\n","from tensorflow.keras.datasets import mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","train_images = train_images.reshape((60000, 28, 28, 1))\n","train_images = train_images.astype(\"float32\") / 255\n","test_images = test_images.reshape((10000, 28, 28, 1))\n","test_images = test_images.astype(\"float32\") / 255\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.fit(train_images, train_labels, epochs=5, batch_size=64)\n","\n","#8-4 컨브넷 평가하기\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(f\"테스트 정확도: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSXi-5ZQKia3","executionInfo":{"status":"ok","timestamp":1712655171797,"user_tz":-540,"elapsed":329499,"user":{"displayName":"방우혁","userId":"03587500139386391947"}},"outputId":"a5e886ee-3bb4-4801-9e2f-c3feb5af60d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","Epoch 1/5\n","938/938 [==============================] - 60s 63ms/step - loss: 0.1570 - accuracy: 0.9504\n","Epoch 2/5\n","938/938 [==============================] - 56s 60ms/step - loss: 0.0445 - accuracy: 0.9863\n","Epoch 3/5\n","938/938 [==============================] - 56s 59ms/step - loss: 0.0301 - accuracy: 0.9906\n","Epoch 4/5\n","938/938 [==============================] - 55s 59ms/step - loss: 0.0227 - accuracy: 0.9929\n","Epoch 5/5\n","938/938 [==============================] - 59s 63ms/step - loss: 0.0179 - accuracy: 0.9947\n","313/313 [==============================] - 3s 10ms/step - loss: 0.0248 - accuracy: 0.9921\n","테스트 정확도: 0.992\n"]}]},{"cell_type":"markdown","source":["평가\n","2장의 완전 연결 네트워크는 n%의 텍스트 정확도를 얻은 반면,\n","기본적인 컨브넷은 99.2%의 테스트 정확도를 얻었음\n","-> 에러율이 상대적으로 60%나 줄었음"],"metadata":{"id":"dIeRwGBHMljr"}},{"cell_type":"markdown","source":["합성곱 신경망\n","합성곱 연산\n","  1. 학습된 패턴은 평행 이동 불변성을 가짐\n","  (입력신호의  특정위치나위치에의존하지않음)\n","  2. 완전 연결 네트워크는 새로운 위치에 나타난 것은 새로운 패턴으로 학습\n"," 3. 적은 수의 훈련 샘플을 사용해서 일반화 능력을 가진 표현을 학습\n","\n","컨브넷은 패턴의 공간적 계층구조를 학습할 수 있음\n","첫번째합성곱층 - 에지같은 작은 지역 패턴을 학습\n","두번째합성곱층 - 첫번째층의 특성으로 구성된 더 큰 패턴을 학습  이런방식을 사용하여 컨브넷은 매우 복잡하고\n","추상적인 시각적 개념을 효과적으로 학습할 수 있음\n","근본적으로 우리가 보는 세상은 공간적 계층 구조를 가지고 있기 때문임"],"metadata":{"id":"puvtx0S0Nc3r"}},{"cell_type":"code","source":["# 8-1 간단한 컨브넷 만들기\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","inputs = keras.Input(shape=(28, 28, 1))\n","x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n","x = layers.Flatten()(x)\n","outputs = layers.Dense(10, activation=\"softmax\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"-st8-3JqOpEC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["입력과동일한높이와너비를가진출력특성맵을얻고싶다면패딩(padding)을  사용할수있음  ⚫패딩은입력특성맵의가장자리에적절한개수의행과열을추가  ⚫모든입력타일에합성곱윈도우의중앙을위치시킬수있음  ⚫하나의행을추가하고오른쪽,왼쪽에하나의열을추가(그림8-6)\n","\n","합성곱 연산\n","합성곱 스트라이드 이해하기\n","⚫ 출력 크기에 영향을 미치는 다른 요소는 스트라이드\n","⚫ 필터를 적용하는 간격을 나타내는 매개변수\n","⚫ 입력 데이터의 너비와 높이에 따라 필터 이동 간격 결정 - strides=(2, 2)\n","⚫ 디폴트 - strides=1\n","⚫ 그림 8-7에서 5×5 크기의 입력(패딩 없음)에 strides=2 를 사용한 3×3 크기의\n","윈도우로 합성곱하여 추출한 패치\n","\n","x = layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='relu')"],"metadata":{"id":"VeU3Mb_xO_Ac"}},{"cell_type":"code","source":["# 8-14. 컨브넷에 추가할 데이터 증식 단계 정의\n","data_augmentation = keras.Sequential(\n","  [\n","      layers.RandomFlip(\"horizontal\"),\n","      layers.RandomRotation(0,1),\n","      layers.RandomZoom(0.2),\n","  ]\n",")\n","\n","# 8-15. 랜덤하게 증식된 훈련 이미지 출력하기\n","plt.figure(figsize=(10, 10))\n","for images, _ in train_dataset.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n","\n","# 8-16 이미지 증식과 드롭아웃을 포함한 컨브넷 만들기\n","inputs = keras.Input(shape=(180, 180, 3))\n","x = data_augmentation(inputs)\n","x = layers.Rescaling(1./255)(x)\n","x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n","x = layers.MaxPooling2D(pool_size=2)(x)\n","x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n","x = layers.Flatten()(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","model.compile(loss=\"binary_crossentropy\",\n","              optimizer=\"rmsprop\",\n","              metrics=[\"accuracy\"])\n","\n","# 8-17. 규제를 추가한 컨브넷 훈련\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n","        save_best_only=True,\n","        monitor=\"val_loss\")\n","]\n","history = model.fit(\n","    train_dataset,\n","    epochs=100,\n","    validation_data=validation_dataset,\n","    callbacks=callbacks\n",")\n","\n","\"\"\"\n"," 결과를 그래프로 나타내 보겠음(그림 8-11)\n","⚫ 데이터 증식과 드롭아웃 덕분에 과대적합이 훨씬 늦은 60~70번째 에포크\n","근처에서 시작 (원본 모델은 10번째 에포크에서 시작)\n","⚫ 검증 정확도는 80~85% 범위에서 유지\n","⚫ 이전 모델보다 훨씬 성능이 좋아졌음\n","\"\"\"\n","\n","# 3-18 테스트 세트에서 모델 훈련, 데이터 증식 사용, 테스트 세트의 정확도 확인\n","test_model = keras.models.load_model(\n","    \"convnet_from_scratch_with_augmentation.keras\")\n","test_loss, test_acc = test_model.evaluate(test_dataset)\n","print(f\"테스트 정확도: {test_acc:.3f}\")"],"metadata":{"id":"fAsRsfFvPSyS","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"error","timestamp":1713252848025,"user_tz":-540,"elapsed":340,"user":{"displayName":"방우혁","userId":"03587500139386391947"}},"outputId":"354f8c05-b57c-4904-ff11-eacb46aa2ef3"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'keras' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-29504b0cfdd4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 8-14. 컨브넷에 추가할 데이터 증식 단계 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data_augmentation = keras.Sequential(\n\u001b[0m\u001b[1;32m      3\u001b[0m   [\n\u001b[1;32m      4\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"horizontal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomRotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Tqzx7I48ylhM"},"execution_count":null,"outputs":[]}]}